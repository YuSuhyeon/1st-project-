"""
ëœí¬ 11% ëª©í‘œí‘œ
ğŸ¯ 2025 ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (ê°œì„  ë²„ì „)
Random Forest, XGBoost, Linear Regression ë¹„êµ
2022-2024 í•™ìŠµ â†’ 2025 ì˜ˆì¸¡ ë° ëª¨ë¸ ì €ì¥
ğŸ”§ ê°œì„ : ì „ì²˜ë¦¬ ì½”ë“œì™€ ì™„ë²½ í˜¸í™˜ + ì„±ëŠ¥ ìµœì í™”
"""

import pandas as pd
import numpy as np
import os
import pickle
import json
from pathlib import Path
from datetime import datetime

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error

from sklearn.model_selection import GridSearchCV

import xgboost as xgb
import joblib

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def train_2025_prediction_models():
    """
    2025 ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
    - Random Forest, XGBoost, Linear Regression ë¹„êµ
    - 2022-2024 í•™ìŠµ â†’ 2025 ì˜ˆì¸¡
    - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì €ì¥
    """
    
    print("ğŸ¯ 2025 ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ! (ê°œì„  ë²„ì „)")
    print("ğŸ¤– ëª¨ë¸: Random Forest, XGBoost, Linear Regression")
    print("ğŸ“š í•™ìŠµ: 2022-2024 â†’ ğŸ”® ì˜ˆì¸¡: 2025")
    print("ğŸ”§ ê°œì„ : ì „ì²˜ë¦¬ ì½”ë“œì™€ ì™„ë²½ í˜¸í™˜ + ì„±ëŠ¥ ìµœì í™”")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ")
    
    try:
        X_train = pd.read_csv('data/processed/X_train.csv')
        y_train = pd.read_csv('data/processed/y_train.csv').squeeze()
        X_predict = pd.read_csv('data/processed/X_predict.csv')
        y_predict = pd.read_csv('data/processed/y_predict.csv').squeeze()
        
        with open('data/processed/mapping_info.pkl', 'rb') as f:
            mapping_info = pickle.load(f)
            
        print(f"   âœ… í•™ìŠµ ë°ì´í„°: {X_train.shape}")
        print(f"   âœ… ì˜ˆì¸¡ ë°ì´í„°: {X_predict.shape}")
        print(f"   âœ… ë§¤í•‘ ì •ë³´ ë¡œë“œ ì™„ë£Œ")
        
    except FileNotFoundError as e:
        print(f"   âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print(f"   ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python 8_feature.py")
        return None
    
    # 2. models í´ë” ìƒì„±
    models_dir = Path("models")
    models_dir.mkdir(exist_ok=True)
    print(f"   ğŸ“ models í´ë” ìƒì„±: {models_dir.absolute()}")
    
    # 3. ë°ì´í„° í™•ì¸
    print("\n2ï¸âƒ£ ë°ì´í„° í™•ì¸")
    
    # ğŸ”§ ê°œì„ : ì „ì²˜ë¦¬ì—ì„œ ì‚¬ìš©í•œ í”¼ì²˜ ì´ë¦„ í™•ì¸
    expected_features = mapping_info.get('feature_names', [])
    if expected_features:
        print(f"   ğŸ“‹ ì „ì²˜ë¦¬ì—ì„œ ì •ì˜í•œ í”¼ì²˜ ({len(expected_features)}ê°œ):")
        for i, feature in enumerate(expected_features, 1):
            print(f"   {i}. {feature}")
    else:
        print(f"   ğŸ“‹ í˜„ì¬ í”¼ì²˜ ({len(X_train.columns)}ê°œ):")
        for i, feature in enumerate(X_train.columns, 1):
            print(f"   {i}. {feature}")
    
    # í”¼ì²˜ ì¼ì¹˜ì„± í™•ì¸
    if set(X_train.columns) == set(expected_features):
        print(f"   âœ… í”¼ì²˜ ì¼ì¹˜ì„± í™•ì¸ ì™„ë£Œ")
    else:
        print(f"   âš ï¸  í”¼ì²˜ ë¶ˆì¼ì¹˜ ê°ì§€ - ê³„ì† ì§„í–‰")
    
    print(f"\n   í•™ìŠµ ë°ì´í„° ê°€ê²© í†µê³„:")
    print(f"   - í‰ê· : {y_train.mean():,.0f}ë§Œì›")
    print(f"   - ì¤‘ì•™ê°’: {y_train.median():,.0f}ë§Œì›")
    print(f"   - ë²”ìœ„: {y_train.min():,.0f} ~ {y_train.max():,.0f}ë§Œì›")
    
    print(f"\n   ì˜ˆì¸¡ ë°ì´í„° ê°€ê²© í†µê³„ (ì •ë‹µ):")
    print(f"   - í‰ê· : {y_predict.mean():,.0f}ë§Œì›")
    print(f"   - ì¤‘ì•™ê°’: {y_predict.median():,.0f}ë§Œì›")
    print(f"   - ë²”ìœ„: {y_predict.min():,.0f} ~ {y_predict.max():,.0f}ë§Œì›")
    
    # ğŸ”§ ê°œì„ : í•™ìŠµ/ì˜ˆì¸¡ ë°ì´í„° ê°€ê²© ì°¨ì´ ë¶„ì„
    price_diff = y_predict.mean() - y_train.mean()
    price_diff_pct = (price_diff / y_train.mean()) * 100
    print(f"\n   ğŸ“ˆ 2025 vs í•™ìŠµ ë°ì´í„° ê°€ê²© ë³€í™”:")
    print(f"   - ì ˆëŒ€ ì°¨ì´: {price_diff:+,.0f}ë§Œì›")
    print(f"   - ìƒëŒ€ ì°¨ì´: {price_diff_pct:+.1f}%")
    
    # 4. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
    print("\n3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥")
    
    results = {}
    predictions = {}
    models_dict = {}  # ğŸ”§ ëª¨ë¸ ê°ì²´ ì €ì¥ìš©
    
    # 4-1. Random Forest (ğŸ”§ ì ë‹¹í•œ ì„±ëŠ¥ìœ¼ë¡œ ì¡°ì •)
    print(f"\n   ğŸŒ² Random Forest í•™ìŠµ...")
    start_time = datetime.now()

    rf_model = RandomForestRegressor(
        # n_estimators=400,      # ğŸ”§ 500 â†’ 350 (ì ë‹¹íˆ ì¤„ì„)
        # max_depth=25,          # ğŸ”§ 25 â†’ 22 (ì‚´ì§ ì¤„ì„)
        # min_samples_split=3,   # ğŸ”§ 3 â†’ 4 (ì‚´ì§ ë³´ìˆ˜ì )
        # min_samples_leaf=1,    # ğŸ”§ 1 â†’ 2 (ì‚´ì§ ë³´ìˆ˜ì )
        # max_features=0.8,   # ğŸ”§ log2 â†’ sqrt (ì‚´ì§ ë³´ìˆ˜ì )
        bootstrap=True,
        oob_score=True,
        random_state=42,
        warm_start=False,
        n_jobs=-1
    )
        
    
    # íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
    param_grid = {
        'n_estimators': [400],
        'max_depth': [25],
        'min_samples_split': [3],
        'min_samples_leaf': [1],
        'max_features': [0.8, 1.0]
    }

    grid_search = GridSearchCV(
        estimator=rf_model,
        param_grid=param_grid,
        cv=5,  # 5ê²¹ êµì°¨ê²€ì¦
        scoring=['neg_mean_squared_error', 'r2', 'neg_mean_absolute_error'],  # íšŒê·€ì—ì„œëŠ” MSE(ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
        refit= 'r2',
        n_jobs=-1,  # ëª¨ë“  CPU ì‚¬ìš©
        verbose=2
        )

    
    best_rf = grid_search.fit(X_train, y_train)
    rf_pred = best_rf.predict(X_predict)
    
    # ì„±ëŠ¥ ê³„ì‚°
    rf_mae = mean_absolute_error(y_predict, rf_pred)
    rf_rmse = np.sqrt(mean_squared_error(y_predict, rf_pred))
    rf_r2 = r2_score(y_predict, rf_pred)
    rf_mape = mean_absolute_percentage_error(y_predict, rf_pred) * 100
    
    train_time = (datetime.now() - start_time).total_seconds()
    
    results['Random Forest'] = {
        'MAE': rf_mae,
        'RMSE': rf_rmse,
        'RÂ²': rf_r2,
        'MAPE': rf_mape,
        'Train Time': train_time
    }
    predictions['Random Forest'] = rf_pred
    models_dict['Random Forest'] = best_rf
    
    # Random Forest ì €ì¥
    rf_path = models_dir / "random_forest_model.pkl"
    joblib.dump(best_rf, rf_path, protocol=4)

    print(f"      ìµœì  ëª¨ë¸ ê°ì²´ : {best_rf.best_estimator_}")
    print(f"   âœ… Random Forest ì™„ë£Œ! ì €ì¥: {rf_path.name}")
    print(f"      MAE: {rf_mae:,.0f}ë§Œì›, RMSE: {rf_rmse:,.0f}ë§Œì›")
    print(f"      RÂ²: {rf_r2:.3f}, MAPE: {rf_mape:.1f}%")
    print(f"      í•™ìŠµ ì‹œê°„: {train_time:.1f}ì´ˆ")
    

    # 4-2. XGBoost (ğŸ”§ ì‚´ì§ ì„±ëŠ¥ í–¥ìƒ)
    print(f"\n   ğŸš€ XGBoost í•™ìŠµ...")
    start_time = datetime.now()
    
    xgb_model = xgb.XGBRegressor(
        n_estimators=250,      # ğŸ”§ 200 â†’ 250 (ì‚´ì§ í–¥ìƒ)
        max_depth=9,           # ğŸ”§ 8 â†’ 9 (ì‚´ì§ í–¥ìƒ)
        learning_rate=0.09,    # ğŸ”§ 0.1 â†’ 0.09 (ì‚´ì§ í–¥ìƒ)
        subsample=0.82,        # ğŸ”§ 0.8 â†’ 0.82 (ì‚´ì§ í–¥ìƒ)
        colsample_bytree=0.82, # ğŸ”§ 0.8 â†’ 0.82 (ì‚´ì§ í–¥ìƒ)
        random_state=42,
        n_jobs=-1,
        verbosity=0
    )
    
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_predict)
    
    # ì„±ëŠ¥ ê³„ì‚°
    xgb_mae = mean_absolute_error(y_predict, xgb_pred)
    xgb_rmse = np.sqrt(mean_squared_error(y_predict, xgb_pred))
    xgb_r2 = r2_score(y_predict, xgb_pred)
    xgb_mape = mean_absolute_percentage_error(y_predict, xgb_pred) * 100
    
    train_time = (datetime.now() - start_time).total_seconds()
    
    results['XGBoost'] = {
        'MAE': xgb_mae,
        'RMSE': xgb_rmse,
        'RÂ²': xgb_r2,
        'MAPE': xgb_mape,
        'Train Time': train_time
    }
    predictions['XGBoost'] = xgb_pred
    models_dict['XGBoost'] = xgb_model
    
    # XGBoost ì €ì¥
    xgb_path = models_dir / "xgboost_model.pkl"
    joblib.dump(xgb_model, xgb_path)
    
    print(f"   âœ… XGBoost ì™„ë£Œ! ì €ì¥: {xgb_path.name}")
    print(f"      MAE: {xgb_mae:,.0f}ë§Œì›, RMSE: {xgb_rmse:,.0f}ë§Œì›")
    print(f"      RÂ²: {xgb_r2:.3f}, MAPE: {xgb_mape:.1f}%")
    print(f"      í•™ìŠµ ì‹œê°„: {train_time:.1f}ì´ˆ")
    
    # 4-3. Linear Regression (ìŠ¤ì¼€ì¼ë§ í•„ìš”)
    print(f"\n   ğŸ“ˆ Linear Regression í•™ìŠµ...")
    start_time = datetime.now()
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_predict_scaled = scaler.transform(X_predict)
    
    lr_model = LinearRegression()
    lr_model.fit(X_train_scaled, y_train)
    lr_pred = lr_model.predict(X_predict_scaled)
    
    # ì„±ëŠ¥ ê³„ì‚°
    lr_mae = mean_absolute_error(y_predict, lr_pred)
    lr_rmse = np.sqrt(mean_squared_error(y_predict, lr_pred))
    lr_r2 = r2_score(y_predict, lr_pred)
    lr_mape = mean_absolute_percentage_error(y_predict, lr_pred) * 100
    
    train_time = (datetime.now() - start_time).total_seconds()
    
    results['Linear Regression'] = {
        'MAE': lr_mae,
        'RMSE': lr_rmse,
        'RÂ²': lr_r2,
        'MAPE': lr_mape,
        'Train Time': train_time
    }
    predictions['Linear Regression'] = lr_pred
    models_dict['Linear Regression'] = (lr_model, scaler)  # ğŸ”§ ìŠ¤ì¼€ì¼ëŸ¬ë„ í•¨ê»˜ ì €ì¥
    
    # Linear Regression & Scaler ì €ì¥
    lr_path = models_dir / "linear_regression_model.pkl"
    scaler_path = models_dir / "scaler.pkl"
    
    joblib.dump(lr_model, lr_path)
    joblib.dump(scaler, scaler_path)
    
    print(f"   âœ… Linear Regression ì™„ë£Œ! ì €ì¥: {lr_path.name}")
    print(f"   âœ… Scaler ì €ì¥: {scaler_path.name}")
    print(f"      MAE: {lr_mae:,.0f}ë§Œì›, RMSE: {lr_rmse:,.0f}ë§Œì›")
    print(f"      RÂ²: {lr_r2:.3f}, MAPE: {lr_mape:.1f}%")
    print(f"      í•™ìŠµ ì‹œê°„: {train_time:.1f}ì´ˆ")
    
    # 5. ì„±ëŠ¥ ë¹„êµ
    print("\n4ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    
    results_df = pd.DataFrame(results).T
    results_df = results_df.round({'MAE': 0, 'RMSE': 0, 'RÂ²': 3, 'MAPE': 1, 'Train Time': 2})
    
    print("\n   ğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
    print(results_df.to_string())
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_model_by_mae = results_df['MAE'].idxmin()
    best_model_by_r2 = results_df['RÂ²'].idxmax()
    best_model_by_mape = results_df['MAPE'].idxmin()

    
    print(f"\n   ğŸ† ìµœê³  ì„±ëŠ¥:")
    print(f"   - MAE ê¸°ì¤€: {best_model_by_mae} ({results_df.loc[best_model_by_mae, 'MAE']:,.0f}ë§Œì›)")
    print(f"   - RÂ² ê¸°ì¤€: {best_model_by_r2} ({results_df.loc[best_model_by_r2, 'RÂ²']:.3f})")
    print(f"   - MAPE ê¸°ì¤€: {best_model_by_mape} ({results_df.loc[best_model_by_mape, 'MAPE']:.1f}%)")
    
    # ğŸ”§ ê°œì„ : ì „ë°˜ì  ìµœê³  ëª¨ë¸ ì„ ì • (MAE + RÂ² ì¢…í•©)
    # MAEëŠ” ë‚®ì„ìˆ˜ë¡, RÂ²ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
    mae_rank = results_df['MAE'].rank(ascending=True)  # ë‚®ì„ìˆ˜ë¡ 1ìœ„
    r2_rank = results_df['RÂ²'].rank(ascending=False)   # ë†’ì„ìˆ˜ë¡ 1ìœ„
    overall_rank = (mae_rank + r2_rank) / 2
    best_overall = overall_rank.idxmin()
    
    print(f"\n   ğŸ¯ ì¢…í•© ìµœê³  ëª¨ë¸: {best_overall}")
    print(f"      MAE: {results_df.loc[best_overall, 'MAE']:,.0f}ë§Œì›")
    print(f"      RÂ²: {results_df.loc[best_overall, 'RÂ²']:.3f}")
    print(f"      MAPE: {results_df.loc[best_overall, 'MAPE']:.1f}%")
    
    # 6. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    print("\n5ï¸âƒ£ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„")
    
    feature_importance = {}
    
    # Random Forest í”¼ì²˜ ì¤‘ìš”ë„
    rf_importance = dict(zip(X_train.columns, best_rf.best_estimator_.feature_importances_))
    feature_importance['Random Forest'] = rf_importance
    
    # XGBoost í”¼ì²˜ ì¤‘ìš”ë„
    xgb_importance = dict(zip(X_train.columns, xgb_model.feature_importances_))
    feature_importance['XGBoost'] = xgb_importance
    
    # Linear Regression ê³„ìˆ˜ (ì ˆëŒ“ê°’)
    lr_coef = dict(zip(X_train.columns, np.abs(lr_model.coef_)))
    # ì •ê·œí™” (0-1 ë²”ìœ„)
    lr_coef_sum = sum(lr_coef.values())
    lr_coef_normalized = {k: v/lr_coef_sum for k, v in lr_coef.items()}
    feature_importance['Linear Regression'] = lr_coef_normalized
    
    # í”¼ì²˜ ì¤‘ìš”ë„ DataFrame ìƒì„±
    importance_df = pd.DataFrame(feature_importance)
    importance_df = importance_df.sort_values(by=['Random Forest', 'XGBoost'], ascending=False)
    
    print(f"\n   ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ (ì „ì²´):")
    print(importance_df.round(3).to_string())
    
    # ğŸ”§ ê°œì„ : ìƒìœ„ í”¼ì²˜ ë¶„ì„
    print(f"\n   ğŸ”¥ ìƒìœ„ 3ê°œ í”¼ì²˜ ë¶„ì„:")
    top_features = importance_df.head(3)
    for i, (feature, row) in enumerate(top_features.iterrows(), 1):
        rf_imp = row['Random Forest']
        xgb_imp = row['XGBoost']
        avg_imp = (rf_imp + xgb_imp) / 2
        print(f"   {i}. {feature}: í‰ê·  ì¤‘ìš”ë„ {avg_imp:.3f}")
        print(f"      RF: {rf_imp:.3f}, XGB: {xgb_imp:.3f}")
    
    # 7. ğŸ”§ ê°œì„ : ëª¨ë¸ë³„ ì—ëŸ¬ ë¶„ì„
    print("\n6ï¸âƒ£ ëª¨ë¸ë³„ ì—ëŸ¬ ë¶„ì„")
    
    for model_name, pred in predictions.items():
        errors = pred - y_predict
        abs_errors = np.abs(errors)
        
        print(f"\n   ğŸ“Š {model_name} ì—ëŸ¬ ë¶„ì„:")
        print(f"   - í‰ê·  ì—ëŸ¬: {errors.mean():+,.0f}ë§Œì›")
        print(f"   - ì—ëŸ¬ í‘œì¤€í¸ì°¨: {errors.std():,.0f}ë§Œì›")
        print(f"   - í° ì—ëŸ¬ (5ì²œë§Œì› ì´ìƒ): {(abs_errors > 50000).sum():,}ê±´ ({(abs_errors > 50000).mean()*100:.1f}%)")
        print(f"   - ì‘ì€ ì—ëŸ¬ (1ì²œë§Œì› ì´í•˜): {(abs_errors <= 10000).sum():,}ê±´ ({(abs_errors <= 10000).mean()*100:.1f}%)")
    
    # 8. ê²°ê³¼ ì €ì¥
    print("\n7ï¸âƒ£ ê²°ê³¼ ì €ì¥")
    
    # JSONìœ¼ë¡œ ì„±ëŠ¥ ê²°ê³¼ ì €ì¥ (ğŸ”§ ê°œì„ : ë” ë§ì€ ì •ë³´ í¬í•¨)
    results_for_json = {}
    for model_name, metrics in results.items():
        results_for_json[model_name] = {
            'MAE': float(metrics['MAE']),
            'RMSE': float(metrics['RMSE']),
            'RÂ²': float(metrics['RÂ²']),
            'MAPE': float(metrics['MAPE']),
            'Train Time': float(metrics['Train Time'])
        }
    
    # ğŸ”§ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    results_for_json['metadata'] = {
        'training_period': mapping_info.get('train_period', '2022-2024'),
        'prediction_period': mapping_info.get('predict_period', '2025'),
        'features_used': mapping_info.get('feature_names', list(X_train.columns)),
        'best_model_overall': best_overall,
        'training_size': int(len(X_train)),
        'prediction_size': int(len(X_predict)),
        'timestamp': datetime.now().isoformat()
    }
    
    results_path = models_dir / "model_results_2025.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results_for_json, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… ì„±ëŠ¥ ê²°ê³¼ ì €ì¥: {results_path.name}")
    
    # í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥
    importance_path = models_dir / "feature_importance_2025.csv"
    importance_df.to_csv(importance_path, encoding='utf-8-sig')
    print(f"   âœ… í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥: {importance_path.name}")
    
    # ğŸ”§ ê°œì„ : ì˜ˆì¸¡ ê²°ê³¼ì— ì—ëŸ¬ ì •ë³´ ì¶”ê°€
    predictions_df = pd.DataFrame(predictions)
    predictions_df['actual'] = y_predict
    
    # ê° ëª¨ë¸ë³„ ì—ëŸ¬ ê³„ì‚°
    for model_name in predictions.keys():
        predictions_df[f'{model_name}_error'] = predictions_df[model_name] - predictions_df['actual']
        predictions_df[f'{model_name}_abs_error'] = np.abs(predictions_df[f'{model_name}_error'])
    
    predictions_path = models_dir / "predictions_2025.csv"
    predictions_df.to_csv(predictions_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {predictions_path.name}")
    
    # ğŸ”§ ë§¤í•‘ ì •ë³´ë„ í•¨ê»˜ ì €ì¥
    mapping_path = models_dir / "mapping_info_copy.pkl"
    with open(mapping_path, 'wb') as f:
        pickle.dump(mapping_info, f)
    print(f"   âœ… ë§¤í•‘ ì •ë³´ ë³µì‚¬: {mapping_path.name}")
    
    # 9. ì‹œê°í™” ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ë” ê¹”ë”í•œ ìŠ¤íƒ€ì¼)
    print("\n8ï¸âƒ£ ì‹œê°í™” ìƒì„±")
    
    # í´ë” ìƒì„±
    plots_dir = Path("plots")
    plots_dir.mkdir(exist_ok=True)
    
    # ğŸ”§ ê°œì„ : ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('default')
    sns.set_palette("husl")
    
    # 9-1. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    models_list = list(results.keys())
    colors = ['#3498db', '#2ecc71', '#e74c3c']  # ğŸ”§ ìƒ‰ìƒ í†µì¼
    
    # MAE ë¹„êµ
    mae_values = [results[model]['MAE'] for model in models_list]
    bars1 = ax1.bar(models_list, mae_values, color=colors)
    ax1.set_title('MAE Comparison (Lower is Better)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('MAE (ë§Œì›)')
    ax1.grid(True, alpha=0.3)
    for i, v in enumerate(mae_values):
        ax1.text(i, v + max(mae_values)*0.01, f'{v:,.0f}', ha='center', va='bottom', fontweight='bold')
    
    # RÂ² ë¹„êµ
    r2_values = [results[model]['RÂ²'] for model in models_list]
    bars2 = ax2.bar(models_list, r2_values, color=colors)
    ax2.set_title('RÂ² Comparison (Higher is Better)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('RÂ²')
    ax2.grid(True, alpha=0.3)
    for i, v in enumerate(r2_values):
        ax2.text(i, v + max(r2_values)*0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # MAPE ë¹„êµ
    mape_values = [results[model]['MAPE'] for model in models_list]
    bars3 = ax3.bar(models_list, mape_values, color=colors)
    ax3.set_title('MAPE Comparison (Lower is Better)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('MAPE (%)')
    ax3.grid(True, alpha=0.3)
    for i, v in enumerate(mape_values):
        ax3.text(i, v + max(mape_values)*0.01, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    # í•™ìŠµ ì‹œê°„ ë¹„êµ
    time_values = [results[model]['Train Time'] for model in models_list]
    bars4 = ax4.bar(models_list, time_values, color=colors)
    ax4.set_title('Training Time Comparison', fontsize=12, fontweight='bold')
    ax4.set_ylabel('Time (seconds)')
    ax4.grid(True, alpha=0.3)
    for i, v in enumerate(time_values):
        ax4.text(i, v + max(time_values)*0.01, f'{v:.1f}s', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    performance_plot_path = plots_dir / "model_performance_comparison.png"
    plt.savefig(performance_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ… ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸: {performance_plot_path.name}")
    
    # 9-2. í”¼ì²˜ ì¤‘ìš”ë„ ì°¨íŠ¸
    fig, axes = plt.subplots(1, 3, figsize=(18, 8))
    
    for i, model_name in enumerate(['Random Forest', 'XGBoost', 'Linear Regression']):
        importance_data = importance_df[model_name].sort_values(ascending=True)
        bars = axes[i].barh(range(len(importance_data)), importance_data.values, color=colors[i])
        axes[i].set_yticks(range(len(importance_data)))
        axes[i].set_yticklabels(importance_data.index)
        axes[i].set_title(f'{model_name}\nFeature Importance', fontsize=12, fontweight='bold')
        axes[i].set_xlabel('Importance')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    importance_plot_path = plots_dir / "feature_importance_comparison.png"
    plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ… í”¼ì²˜ ì¤‘ìš”ë„ ì°¨íŠ¸: {importance_plot_path.name}")
    
    # 9-3. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„ (ğŸ”§ ê°œì„ : ë” ì˜ˆìœ ìŠ¤íƒ€ì¼)
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for i, model_name in enumerate(models_list):
        y_pred = predictions[model_name]
        axes[i].scatter(y_predict, y_pred, alpha=0.6, s=2, color=colors[i])
        
        # ì™„ë²½í•œ ì˜ˆì¸¡ì„ 
        min_val = min(y_predict.min(), y_pred.min())
        max_val = max(y_predict.max(), y_pred.max())
        axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, alpha=0.8)
        
        axes[i].set_xlabel('Actual Price (ë§Œì›)')
        axes[i].set_ylabel('Predicted Price (ë§Œì›)')
        axes[i].set_title(f'{model_name}\nRÂ² = {results[model_name]["RÂ²"]:.3f}, MAE = {results[model_name]["MAE"]:,.0f}', 
                         fontsize=11, fontweight='bold')
        axes[i].grid(True, alpha=0.3)
        
        # ì¶• ë²”ìœ„ ë™ì¼í•˜ê²Œ ì„¤ì •
        axes[i].set_xlim(min_val, max_val)
        axes[i].set_ylim(min_val, max_val)
    
    plt.tight_layout()
    scatter_plot_path = plots_dir / "prediction_vs_actual_scatter.png"
    plt.savefig(scatter_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ… ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„: {scatter_plot_path.name}")
    
    # ğŸ”§ ì¶”ê°€: ì—ëŸ¬ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for i, model_name in enumerate(models_list):
        errors = predictions[model_name] - y_predict
        axes[i].hist(errors, bins=50, alpha=0.7, color=colors[i], edgecolor='black')
        axes[i].axvline(0, color='red', linestyle='--', linewidth=2)
        axes[i].set_xlabel('Prediction Error (ë§Œì›)')
        axes[i].set_ylabel('Frequency')
        axes[i].set_title(f'{model_name}\nError Distribution\nMean: {errors.mean():+,.0f}ë§Œì›', 
                         fontsize=11, fontweight='bold')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    error_plot_path = plots_dir / "error_distribution.png"
    plt.savefig(error_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ… ì—ëŸ¬ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨: {error_plot_path.name}")
    
    # 10. ì €ì¥ëœ íŒŒì¼ í™•ì¸
    print("\n9ï¸âƒ£ ì €ì¥ëœ íŒŒì¼ í™•ì¸")
    
    print(f"\n   ğŸ’¾ Models í´ë”:")
    for file_path in sorted(models_dir.glob("*")):
        file_size = file_path.stat().st_size / (1024*1024)  # MB
        print(f"   âœ… {file_path.name} ({file_size:.1f}MB)")
    
    print(f"\n   ğŸ“Š Plots í´ë”:")
    for file_path in sorted(plots_dir.glob("*")):
        print(f"   âœ… {file_path.name}")
    
    # 11. ğŸ”§ ê°œì„ : ëª¨ë¸ ì¶”ì²œ
    print("\nğŸ”Ÿ ëª¨ë¸ ì¶”ì²œ")
    
    print(f"\n   ğŸ¯ ìš©ë„ë³„ ëª¨ë¸ ì¶”ì²œ:")
    
    # ì •í™•ë„ ìš°ì„ 
    best_accuracy = results_df.loc[results_df['RÂ²'].idxmax()]
    print(f"   ğŸ“ˆ ì •í™•ë„ ìš°ì„ : {results_df['RÂ²'].idxmax()}")
    print(f"      â†’ RÂ² {best_accuracy['RÂ²']:.3f}, MAE {best_accuracy['MAE']:,.0f}ë§Œì›")
    
    # ì†ë„ ìš°ì„   
    best_speed = results_df.loc[results_df['Train Time'].idxmin()]
    print(f"   âš¡ ì†ë„ ìš°ì„ : {results_df['Train Time'].idxmin()}")
    print(f"      â†’ í•™ìŠµì‹œê°„ {best_speed['Train Time']:.1f}ì´ˆ, RÂ² {best_speed['RÂ²']:.3f}")
    
    # ê· í˜• ìš°ì„  (ì¢…í•©)
    print(f"   âš–ï¸  ê· í˜• ìš°ì„ : {best_overall}")
    print(f"      â†’ ì •í™•ë„ì™€ ì•ˆì •ì„±ì˜ ìµœì  ì¡°í•©")
    
    # 12. ğŸ”§ ê°œì„ : ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ
    print(f"\n1ï¸âƒ£1ï¸âƒ£ ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ")
    
    print(f"\n   ğŸ’¡ ëª¨ë¸ í™œìš© íŒ:")
    print(f"   1. ë‹¨ì¼ ì˜ˆì¸¡: {best_overall} ëª¨ë¸ ì‚¬ìš© ê¶Œì¥")
    print(f"   2. ëŒ€ëŸ‰ ì˜ˆì¸¡: Random Forest (ë¹ ë¥¸ ì†ë„)")
    print(f"   3. ì•™ìƒë¸”: ìƒìœ„ 2ê°œ ëª¨ë¸ í‰ê·  í™œìš©")
    print(f"   4. ì‹ ë¢°êµ¬ê°„: Â±{results_df.loc[best_overall, 'MAE']:,.0f}ë§Œì› ê³ ë ¤")
    
    print(f"\n   ğŸ“‹ ì£¼ì˜ì‚¬í•­:")
    print(f"   - 2025ë…„ ì´í›„ ë°ì´í„°ì—” ì¬í•™ìŠµ í•„ìš”")
    print(f"   - ê·¹ë‹¨ì  í‰ìˆ˜/ê°€ê²© ì…ë ¥ ì‹œ ì •í™•ë„ í•˜ë½")
    print(f"   - ì‹ ê·œ ë¸Œëœë“œëŠ” 'ë¸Œëœë“œì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬")
    
    # 13. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ‰ 2025 ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ê°œì„  ë²„ì „)")
    print(f"ğŸ† ì¢…í•© ìµœê³  ëª¨ë¸: {best_overall}")
    print(f"ğŸ“Š ìµœê³  ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   - MAE: {results_df.loc[best_overall, 'MAE']:,.0f}ë§Œì›")
    print(f"   - RÂ²: {results_df.loc[best_overall, 'RÂ²']:.3f}")
    print(f"   - MAPE: {results_df.loc[best_overall, 'MAPE']:.1f}%")
    print(f"ğŸ”§ ê°œì„ ì‚¬í•­:")
    print(f"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ")
    print(f"   - ì—ëŸ¬ ë¶„ì„ ë° ì‹œê°í™” ì¶”ê°€")
    print(f"   - ì‹¤ë¬´ í™œìš© ê°€ì´ë“œ ì œê³µ")
    print(f"   - ì¢…í•© ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    return {
        'results': results_df,
        'predictions': predictions_df,
        'feature_importance': importance_df,
        'best_models': {
            'MAE': best_model_by_mae,
            'RÂ²': best_model_by_r2,
            'MAPE': best_model_by_mape,
            'Overall': best_overall
        },
        'models': models_dict,
        'mapping_info': mapping_info
    }

if __name__ == "__main__":
    print("ğŸ¯ 2025 ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (ê°œì„  ë²„ì „)")
    print("ğŸ¤– Random Forest, XGBoost, Linear Regression")
    print("ğŸ“š 2022-2024 í•™ìŠµ â†’ ğŸ”® 2025 ì˜ˆì¸¡")
    print("ğŸ”§ ê°œì„ : ì „ì²˜ë¦¬ ì½”ë“œì™€ ì™„ë²½ í˜¸í™˜ + ì„±ëŠ¥ ìµœì í™”")
    print()
    
    result = train_2025_prediction_models()
    
    if result:
        print(f"\nğŸŠ ëª¨ë¸ í•™ìŠµ ì„±ê³µ! ğŸŠ")
        print(f"ì €ì¥ëœ ëª¨ë¸ë¡œ ì‹¤ì œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•˜ì„¸ìš”!")
        print(f"ê¶Œì¥ ëª¨ë¸: {result['best_models']['Overall']}")
    else:
        print(f"\nâŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")